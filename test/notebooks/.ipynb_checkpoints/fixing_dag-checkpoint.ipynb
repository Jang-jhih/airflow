{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40645a89-b814-4556-858e-63491954012b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=997f7cb1c14ba52ff9f4991f9c8319e668243234d7d7c3fa637c161417b02435\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca20841-052f-4525-8cf6-7ce6eb75ee22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.rrule import rrule, DAILY, MONTHLY\n",
    "import os\n",
    "# from tqdm import tnrange, tqdm_notebook\n",
    "import pickle\n",
    "import sqlalchemy\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format\n",
    "# from textwrap import dedent\n",
    "from airflow import DAG \n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from finance.stock import (crawl_price,crawl_bargin,crawl_benchmark,crawl_pe)\n",
    "import time\n",
    "from finance.process import test_database, date_range\n",
    "import os\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.models import Variable\n",
    "from hdfs import InsecureClient\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import tempfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081b0531-7f48-4748-b916-2b77b9ab22a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawlar price 2019-01-07\n"
     ]
    }
   ],
   "source": [
    "tablename='price'\n",
    "crawl_func =crawl_price\n",
    "datetime_object = datetime.strptime('20190107', '%Y%m%d')\n",
    "\n",
    "dates = date_range(datetime_object, datetime.now())\n",
    "date = dates[0]\n",
    "\n",
    "print(f'Crawlar {tablename} {date}')\n",
    "\n",
    "df = crawl_func(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419f7da6-eeb7-4d1c-96bb-263862a0ee16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/07 15:32:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SaveToParquet\")\n",
    "    # .master(f\"local[{cores}]\")  # 使用动态检测到的核心数\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "hdfs_path = f'hdfs://namenode:8020/datawarehouse/{tablename}'\n",
    "\n",
    "\n",
    "new_data_df=df\n",
    "new_data_df = new_data_df.sort_values(by=['date'])\n",
    "new_data_df = spark.createDataFrame(new_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa084bf6-8694-4589-bfc3-ab68e15c1c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
